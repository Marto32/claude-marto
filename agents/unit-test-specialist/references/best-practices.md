# Universal Unit Testing Best Practices

This guide covers language-agnostic principles and patterns for effective unit testing.

## Core Principles

### FIRST Principles

**F - Fast**
- Tests should run in milliseconds, not seconds
- Use in-memory databases (SQLite, H2)
- Mock slow external dependencies
- Avoid file I/O when possible
- Target: < 1 second per test

**I - Independent/Isolated**
- Each test should run in complete isolation
- No shared mutable state between tests
- No dependencies on test execution order
- Tests should pass when run individually or in suite

**R - Repeatable**
- Tests should produce same result every time
- No dependency on external systems (network, database)
- No randomness without seeded values
- No time-based dependencies without mocking

**S - Self-Validating**
- Tests should have clear pass/fail result
- No manual verification needed
- Use assertions, not print statements
- Clear, descriptive failure messages

**T - Timely**
- Write tests when writing code (TDD) or immediately after
- Don't delay testing until "later"
- Tests should be written by same person who writes code

## Test Structure

### AAA Pattern (Arrange-Act-Assert)

```
test_name:
  # Arrange: Set up test data and environment
  given_some_context

  # Act: Execute the code under test
  when_action_occurs

  # Assert: Verify the expected outcome
  then_expect_result
```

**Example:**
```python
def test_user_registration_with_valid_data():
    # Arrange
    user_data = {"email": "test@example.com", "password": "secure123"}
    service = UserService(mock_repository)

    # Act
    result = service.register_user(user_data)

    # Assert
    assert result.success is True
    assert result.user.email == "test@example.com"
```

### Given-When-Then (BDD Style)

```
Scenario: User registration with valid credentials
  Given a new user with valid email and password
  When the user attempts to register
  Then the registration should succeed
  And the user should receive a confirmation email
```

## Test Naming

### Descriptive Names

**Pattern:** `test_<scenario>_<expected_outcome>`

**Good Examples:**
- `test_user_creation_with_duplicate_email_raises_validation_error`
- `test_authentication_with_invalid_password_returns_false`
- `test_order_total_calculation_includes_tax_and_shipping`
- `test_async_operation_timeout_raises_timeout_error`

**Bad Examples:**
- `test_user` (too vague)
- `test_1`, `test_2` (no context)
- `test_works` (not descriptive)
- `test_bug_fix` (what bug?)

### Test Method Naming Conventions by Language

- **Python:** `test_lowercase_with_underscores`
- **JavaScript/TypeScript:** `should do something when condition`
- **Java/Kotlin:** `shouldDoSomethingWhenCondition` or `testMethodName_StateUnderTest_ExpectedBehavior`
- **Go:** `TestFunctionName_StateUnderTest`
- **C++:** `TestCaseName_TestName`

## What to Test

### High Priority
1. **Critical paths** - Authentication, payments, data integrity
2. **Business logic** - Core functionality, calculations, validations
3. **Edge cases** - Boundary conditions, null/empty values
4. **Error handling** - Exception paths, validation failures
5. **Public interfaces** - API contracts, class interfaces

### Medium Priority
6. **Utility functions** - Formatters, converters, helpers
7. **Data transformations** - Mappers, serializers
8. **Conditional logic** - All branches of if/switch statements

### Low Priority (Often Skip)
9. **Trivial getters/setters** - Simple property access
10. **Framework code** - Third-party library behavior
11. **Configuration** - Static configuration files
12. **Generated code** - Auto-generated by tools

## Mocking Strategy

### What to Mock

**Mock at boundaries:**
- HTTP clients and API calls
- Database connections
- File system operations
- External services (email, payment processors)
- Time/date functions
- Random number generators

**Don't mock:**
- Internal business logic
- Simple data structures
- Value objects
- Pure functions without side effects

### Mock vs Stub vs Fake vs Spy

**Mock:** Pre-programmed object with expectations about calls
```python
mock_service = Mock()
mock_service.get_user.return_value = user
# Later: verify it was called
mock_service.get_user.assert_called_once_with(user_id)
```

**Stub:** Object with canned responses, no verification
```python
stub_service = Stub()
stub_service.get_user = lambda id: User(id=id)
# No verification of calls
```

**Fake:** Working implementation, but simplified
```python
class FakeDatabase:
    def __init__(self):
        self.data = {}  # In-memory storage

    def save(self, key, value):
        self.data[key] = value

    def get(self, key):
        return self.data.get(key)
```

**Spy:** Real object that records how it's used
```python
spy = Spy(real_object)
spy.method()  # Calls real method
# Later: verify it was called
assert spy.method_was_called()
```

### Prefer Fakes Over Mocks When Possible

Fakes are more maintainable and catch more bugs:

```python
# Good: In-memory fake
database = InMemoryDatabase()
repository = UserRepository(database)
service = UserService(repository)

# Less ideal: Heavy mocking
mock_db = Mock()
mock_repo = Mock()
mock_repo.save.return_value = True
# ... lots of mock setup
```

## Test Data Management

### Factories

**Pattern:** Create test objects with sensible defaults

```python
def create_user(**overrides):
    defaults = {
        "id": generate_id(),
        "email": generate_email(),
        "username": generate_username(),
        "created_at": datetime.now(),
    }
    return User(**{**defaults, **overrides})

# Usage
user = create_user(email="specific@example.com")
admin = create_user(is_admin=True)
```

### Builders

**Pattern:** Fluent interface for complex objects

```python
user = UserBuilder()
    .with_email("test@example.com")
    .with_role("admin")
    .with_verified_email()
    .build()
```

### Fixtures

**Pattern:** Reusable test data setup

```python
@pytest.fixture
def sample_users():
    return [
        create_user(username="user1"),
        create_user(username="user2"),
        create_user(username="user3"),
    ]

def test_user_list(sample_users):
    assert len(sample_users) == 3
```

### Mother Pattern

**Pattern:** Factory with predefined meaningful configurations

```python
class UserMother:
    @staticmethod
    def admin():
        return create_user(is_admin=True, permissions=["all"])

    @staticmethod
    def regular_user():
        return create_user(is_admin=False, permissions=["read"])

    @staticmethod
    def suspended_user():
        return create_user(is_suspended=True, suspended_at=datetime.now())
```

## Parameterized Testing

### Benefits
- Reduces code duplication
- Tests multiple scenarios with same logic
- Clear documentation of edge cases
- Easy to add new test cases

### When to Use
- Testing same logic with different inputs
- Boundary value testing
- Testing multiple error conditions
- Cross-browser/cross-platform testing

### Pattern
```python
@pytest.mark.parametrize("input,expected", [
    ("", ""),                    # Empty string
    ("hello", "HELLO"),          # Lowercase
    ("HELLO", "HELLO"),          # Already uppercase
    ("HeLLo", "HELLO"),          # Mixed case
    ("hello123", "HELLO123"),    # Alphanumeric
    ("hello world", "HELLO WORLD"),  # Spaces
    ("ÑOÑO", "ÑOÑO"),           # Unicode
])
def test_uppercase(input, expected):
    assert uppercase(input) == expected
```

## Test Organization

### File Structure

**Mirror source code structure:**
```
src/
  users/
    user.service.js
    user.repository.js
    user.validator.js
tests/
  users/
    user.service.test.js
    user.repository.test.js
    user.validator.test.js
```

### Grouping Tests

**By functionality:**
```
describe("UserService", () => {
  describe("registration", () => {
    test("with valid data succeeds");
    test("with duplicate email fails");
    test("with invalid email fails");
  });

  describe("authentication", () => {
    test("with correct password succeeds");
    test("with wrong password fails");
    test("with non-existent user fails");
  });
});
```

### Test Categories

**Use markers/tags for categorization:**

```python
@pytest.mark.smoke
@pytest.mark.fast
def test_critical_feature():
    pass

@pytest.mark.slow
def test_complex_calculation():
    pass

@pytest.mark.integration  # Rarely, should mostly be unit
def test_database_integration():
    pass
```

**Run specific categories:**
```bash
# Fast tests only (for quick feedback)
pytest -m "not slow"

# Smoke tests (critical functionality)
pytest -m smoke

# All tests except integration
pytest -m "not integration"
```

## Async Testing

### Testing Async Functions

```python
@pytest.mark.asyncio
async def test_async_operation():
    result = await async_function()
    assert result == expected
```

### Testing Timeouts

```python
async def test_operation_timeout():
    with pytest.raises(asyncio.TimeoutError):
        await asyncio.wait_for(slow_operation(), timeout=0.1)
```

### Testing Concurrent Operations

```python
async def test_concurrent_requests():
    results = await asyncio.gather(
        request1(),
        request2(),
        request3(),
    )
    assert all(r.success for r in results)
```

### Mocking Async Functions

```python
mock_fetch = AsyncMock(return_value=data)
# or
mock_fetch = Mock(return_value=asyncio.Future())
mock_fetch.return_value.set_result(data)
```

## CI/CD Optimization

### Keep Tests Fast

**Target execution times:**
- Unit test: < 1 second
- Full test suite: < 5 minutes (ideally)
- Smoke tests: < 30 seconds

**Techniques:**
- Use in-memory databases
- Mock external dependencies
- Parallelize test execution
- Cache expensive setup
- Avoid unnecessary file I/O

### Parallel Execution

**Enable parallelization:**
```bash
# Python (pytest-xdist)
pytest -n auto

# JavaScript (Jest)
jest --maxWorkers=4

# Go
go test -parallel 4
```

**Requirements:**
- Tests must be independent
- No shared mutable state
- No file system conflicts
- Process-level isolation if needed

### Test Categorization for CI

```yaml
# CI Pipeline Example
stages:
  - smoke:    # Run on every commit (<30s)
      run: tests marked as "smoke"
  - fast:     # Run on pre-merge (<2min)
      run: tests not marked as "slow"
  - full:     # Run on PR merge (<5min)
      run: all tests
  - nightly:  # Run daily
      run: all tests + slow tests
```

## Code Coverage

### Coverage Goals

**Targets:**
- Critical paths: 100%
- Business logic: 95%+
- Utilities: 90%+
- Total project: 95%+

### What Coverage Doesn't Mean

❌ **100% coverage ≠ bug-free code**
- Coverage measures lines executed, not correctness
- Can have high coverage with poor assertions
- Edge cases might not be tested

✅ **Good coverage means:**
- Most code paths are exercised
- Developers have thought about the code
- Regressions are more likely to be caught

### Branch Coverage

More valuable than line coverage:
```python
def discount(price, is_member):
    # Line coverage might miss the False branch
    if is_member:
        return price * 0.9
    return price

# Need tests for both is_member=True and is_member=False
```

### Excluding Code from Coverage

```python
def debug_function():  # pragma: no cover
    print("Debug information")

if TYPE_CHECKING:  # pragma: no cover
    from typing import TypeAlias
```

## Common Anti-Patterns

### ❌ Testing Implementation Details

**Bad:**
```python
def test_internal_state():
    obj = MyClass()
    obj._private_method()
    assert obj._internal_state == "value"
```

**Good:**
```python
def test_public_behavior():
    obj = MyClass()
    result = obj.public_method()
    assert result == expected_output
```

### ❌ Overly Complex Tests

**Bad:**
```python
def test_complex():
    # 50 lines of setup
    # Multiple conditionals
    # Loops
    # Nested assertions
    # Hard to understand what's being tested
```

**Good:**
```python
def test_specific_behavior():
    # Clear setup
    user = create_user(status="active")
    # Clear action
    result = service.deactivate_user(user.id)
    # Clear assertion
    assert result.status == "inactive"
```

### ❌ Test Interdependence

**Bad:**
```python
class TestSuite:
    user = None

    def test_create_user(self):
        self.user = create_user()  # Shared state!

    def test_update_user(self):
        update_user(self.user)  # Depends on previous test!
```

**Good:**
```python
class TestSuite:
    def test_create_user(self):
        user = create_user()
        assert user.id is not None

    def test_update_user(self):
        user = create_user()  # Independent setup
        updated = update_user(user.id, new_data)
        assert updated.name == new_data.name
```

### ❌ Testing Multiple Things in One Test

**Bad:**
```python
def test_everything():
    user = create_user()
    assert user.id is not None
    assert user.email is not None
    updated = update_user(user)
    assert updated.name == "new"
    deleted = delete_user(user)
    assert deleted is True
```

**Good:**
```python
def test_user_creation_assigns_id():
    user = create_user()
    assert user.id is not None

def test_user_update_changes_name():
    user = create_user()
    updated = update_user(user.id, name="new")
    assert updated.name == "new"

def test_user_deletion_returns_true():
    user = create_user()
    result = delete_user(user.id)
    assert result is True
```

### ❌ Mocking Everything

**Bad:**
```python
mock_a = Mock()
mock_b = Mock()
mock_c = Mock()
mock_d = Mock()
# ... lots of mock setup
service = MyService(mock_a, mock_b, mock_c, mock_d)
# Test becomes fragile and doesn't test real behavior
```

**Good:**
```python
# Only mock external boundaries
real_validator = Validator()
real_formatter = Formatter()
mock_api_client = Mock()  # External dependency

service = MyService(mock_api_client, real_validator, real_formatter)
```

## Debugging Test Failures

### Strategies

1. **Read the failure message carefully**
   - What was expected vs actual?
   - Which assertion failed?
   - Stack trace location

2. **Isolate the failing test**
   - Run only that test
   - Check for test interdependence
   - Verify test data setup

3. **Add debugging output**
   - Print intermediate values
   - Log mock call arguments
   - Check state before assertions

4. **Reproduce locally**
   - Same environment as CI
   - Same data/fixtures
   - Same test execution order

5. **Check for flakiness**
   - Run test multiple times
   - Look for timing issues
   - Check for external dependencies

### Flaky Test Causes

- Time/date dependencies
- Random values without seeding
- Async timing issues
- External service dependencies
- Shared mutable state
- File system state
- Test execution order dependency

## Documentation Through Tests

### Tests as Examples

Good tests serve as usage documentation:

```python
def test_user_registration_complete_example():
    """
    Example of complete user registration flow.

    This test demonstrates:
    - Creating a user with required fields
    - Validating email format
    - Hashing password securely
    - Sending confirmation email
    """
    user_data = {
        "email": "user@example.com",
        "password": "SecurePass123!",
        "name": "John Doe",
    }

    result = register_user(user_data)

    assert result.success
    assert result.user.email == "user@example.com"
    assert result.user.password != "SecurePass123!"  # Hashed
    assert result.confirmation_email_sent
```

### Test Names Tell Stories

```python
# These test names document the business rules
def test_premium_users_get_20_percent_discount()
def test_orders_over_100_dollars_get_free_shipping()
def test_inactive_users_cannot_post_comments()
def test_passwords_must_contain_special_character()
```

## Summary Checklist

**Before Writing Tests:**
- [ ] Understand the requirement
- [ ] Identify the unit to test
- [ ] List all scenarios (happy path + edge cases)
- [ ] Plan mocking strategy

**While Writing Tests:**
- [ ] Use AAA pattern
- [ ] Write descriptive test names
- [ ] Keep tests simple and focused
- [ ] Mock only external dependencies
- [ ] Use factories for test data
- [ ] Test edge cases and errors

**After Writing Tests:**
- [ ] Tests are independent
- [ ] Tests run fast (< 1 second)
- [ ] Coverage meets targets (95%+)
- [ ] Tests are readable
- [ ] No flaky tests
- [ ] CI pipeline passes

**Good Unit Tests Are:**
- ✅ Fast (milliseconds)
- ✅ Isolated (independent)
- ✅ Repeatable (deterministic)
- ✅ Self-validating (clear pass/fail)
- ✅ Thorough (95%+ coverage)
- ✅ Maintainable (easy to update)
- ✅ Readable (clear intent)
